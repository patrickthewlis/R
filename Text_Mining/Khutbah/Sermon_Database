###############################
 Specify Path in normalizePath
###############################

# Allowing us to run our script
        # Defines the library trees where are packages are looked for
        # All of the packages are stored in one folder

.libPaths("S:/R/win-library/3.4")


################
  Libraries
################ 

# Install at once using the 'c' array function

lapply(c("XML", "rvest", "dplyr", "tidytext", "tm", "openNLP", "stringr", "ggplot2"),
       library, character.only = TRUE)

################
 Loading Texts
################

texts <- file.path("S:", "Python", "Text_Files", "Sermons") # Construct path to file
texts
dir(texts)                                                  # Check that the corpus has loaded


####################
 Load Texts into R
####################

# Using the "tm" package
      # Structure for managing documents in tm = CORPUS
        # Command: "VCorpus"

docs <- VCorpus(DirSource(texts))     # Identify the source of the directory
                                      # i.e. document within file directory
summary(docs)                         # Summarise the corpus

writeLines(as.character(docs))        # Read the document in R Terminal




######################################################################
#                   PRE-PROCESSING- tm v0.5/v0.6
######################################################################

# FOR ALL PRE-PROCESSING, NEED TO USE "content_transforer()"


#####################
       Removal
#####################

# Command: textfile <- tm_map(textfile, removeFunction)


# Remove Stopwords

        # Custom stopwords
        myStopwords <- c(stopwords('english'), "muslims", "wa", "al", "ala",
                         "o", "s", "go", "back", "two", "says", "can", "days",
                         "yet", "used", "without", "sws", "many", "except",
                         "ever", "even", "another", "example", "came", "however",
                         "every", "name", "said", "please", "shall", "see", "allÃ¡h",
                         "allahs", "also", "one", "now", "new", "never", "take", "may",
                         "today", "say", "dont", "day")
                         
docs <- tm_map(docs, content_transformer(removeWords), myStopwords)


# Convert all words to lowercase

docs <- tm_map(docs, content_transformer(tolower))

# Remove URLs

    # define function to remove URL
    # Substitute all lines starting with "http" with blankspace

removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
    
    # argument = function (x)
    # Therefore, when we are working with "docs", x = docs
    # Similar to when f(x)=x2, when f(5)=25
    # Therefore, wherever x is in the function, it is replaced by "docs"
    # Thus, gsub("http[^[:space:]]*", "", x) = gsub("http[^[:space:]]*", "", docs)
    
    # Therefore removeURL(docs) is equal to gsub("http[^[:space:]]*", "", docs)

    
docs <- tm_map(docs, content_transformer(removeURL)) # Put another way
                                # docs <- tm_map(docs, (gsub"http[^[:space:]]*", "", docs)))


# Remove anything OTHER than English

    # define function
    # Keep if alpha (i.e. letters) and space
    # Everything else: Remove

removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)

docs <- tm_map(docs, content_transformer(removeNumPunct))

        

# Check that pre-processing has worked

writeLines(as.character(docs))

# Create a copy of the original

docs_copy <- docs







####################################################################
#                   Tokenizing and Searching
####################################################################

# Create Term-Document Matrix

tdm <- TermDocumentMatrix(docs,
                          control = list(wordLengths = c(1, Inf))) # No restrictions on wordlength
                                                                   # "inf" denotes infinity

tdm                                                                # Inspect Term-Document Matrix


# Inspect Frequency of Words
        # command: 'findFreqTerms(tdm)'

(freq.terms <- findFreqTerms(tdm, lowfreq = 750))  # lowfreq = 750
                                                   # lower frequency bound is 750
                                                   # Therefore, print only words that appear more than 1000 times 


# Find frequency for all words in the corpus
        # Treat Term-Document Matrix as a matrix i.e. as it already is

term.freq <- rowSums(as.matrix(tdm))

# Create/return subset of word vectors which meet set conditions
        # Conditions: Must appear 750 or more times in the corpus

term.freq <- subset(term.freq, term.freq >=750)

# Store the subset in a dataframe for 'tidy' text analysis

dataframe <- data.frame(term = names(term.freq), freq = term.freq)






####################################################################
#                   Visualising Word Distributions
####################################################################

# Package: ggplot2
# Bar Graph: Frequency Distribution
    # Count Model
    # Visual Preferences: Fill in Red with Black Border
    # Count intervals are set to 555 words i.e. 'scale_y_continuous

ggplot(dataframe, aes(x = term, y = freq)) + geom_bar(stat = "identity", fill="#FF9999", colour="black") +
    xlab("Words") + ylab("Count") + coord_flip() +
    scale_y_continuous(breaks = round(seq(min(dataframe$freq), max(dataframe$freq), by = 555)))

